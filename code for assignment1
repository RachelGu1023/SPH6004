
#imput libraries
import numpy as np 
import pandas as pd 
from sklearn import preprocessing
import matplotlib.pyplot as plt 
plt.rc("font", size=14)
import seaborn as sns


#imput Assignment data
dataset = pd.read_csv("/Users/tygu/Documents/Singapore/PhD/course/AY2223-S2/SPH6004/Assignment 1/Assignment_1_data.csv")
dataset.head()
print('data set includes {} participants.'.format(dataset.shape[0]))
#check missing value
dataset.info()
dataset.isnull().sum()


# how many total missing values do we have?
# get the number of missing data points per column
missing_values_count = dataset.isnull().sum()
total_cells = np.product(dataset.shape)
total_missing = dataset.isnull().sum()

# percent of data that is missing
(total_missing/total_cells) * 10


#checking missing percentages features    
def missing(datasest):
    print("Missing values in %")
    print(round((dataset.isnull().sum() * 100/ len(dataset)),2).sort_values(ascending=False))
    
missing(dataset)


#drop features if missing percentage > 0.75
data = dataset.copy()
data = data.drop(columns= [x for x in dataset if round((dataset[x].isna().sum()/len(data)*100),2) > 75 ])
#check missing value
data.info()
data.isnull().sum()



#define missing data function
def missing(data):
    print("missing value %")
    print(round((data.isnull().sum() * 100/ len(data)),2).sort_values(ascending=False))
    
missing(data)




#data pre-processing
#change the missing categorical data to the max frequency
#change the missing continuous data to mean or median if normal/not normal
# check sofa_renal feature, and replace NA as most count 0
print('sofa renal distribution (0=0, 1=1.0, 2=2.0, 3=3.0, 4 = 4.0):')
print(data['sofa_renal'].value_counts())
sns.countplot(x='sofa_renal', data=data, palette='Set2')
plt.show()

data["sofa_renal"].fillna(data['sofa_renal'].value_counts().idxmax(), inplace=True)


# check sofa_cns feature, and replace NA as most count 0
print('sofa cns distribution (0=0, 1=1.0, 2=2.0, 3=3.0, 4 = 4.0):')
print(data['sofa_cns'].value_counts())
#sns.countplot(x='sofa_cns', data=data, palette='Set2')
#plt.show()

data["sofa_cns"].fillna(data['sofa_cns'].value_counts().idxmax(), inplace=True)



#check heart rate mean
plot= data["heart_rate_mean"].hist(bins=15, color='teal')
plot.set(xlabel='heart_rate_mean')
plt.xlim(0,200)
plt.show()

skewness = data["heart_rate_mean"].skew()
skewness1 = data["heart_rate_min"].skew()
skewness2 = data["heart_rate_max"].skew()
skewness
skewness1
skewness2

#replace NA in heart rate mean/max/min as mean because it is not normal distributed
data["heart_rate_mean"].fillna(data["heart_rate_mean"].median(skipna=True), inplace=True)
data["heart_rate_max"].fillna(data["heart_rate_max"].median(skipna=True), inplace=True)
data["heart_rate_min"].fillna(data["heart_rate_min"].median(skipna=True), inplace=True)


# check sofa_cardiovascular feature, and replace NA as most count 0
print('sofa cardiovascular distribution (0=0, 1=1.0, 2=2.0, 3=3.0, 4 = 4.0):')
print(data['sofa_cardiovascular'].value_counts())
sns.countplot(x='sofa_cardiovascular', data=data, palette='Set2')
plt.show()

data["sofa_cardiovascular"].fillna(data['sofa_cardiovascular'].value_counts().idxmax(), inplace=True)



#check mbp
plot= data["mbp_min"].hist(bins=15, color='teal')
plot.set(xlabel='mbp_min')
plt.xlim(0,200)
plt.show()


#replace NA in mbp mean/max/min as meidan because it is not normal distributed
data["mbp_mean"].fillna(data["mbp_mean"].median(skipna=True), inplace=True)
data["mbp_max"].fillna(data["mbp_max"].median(skipna=True), inplace=True)
data["mbp_min"].fillna(data["mbp_min"].median(skipna=True), inplace=True)

#check sbp
plot = data["sbp_min"].hist(bins=15, color='teal')
plot.set(xlabel='sbp_min')
plt.xlim(0,200)
plt.show()

#replace NA in sbp mean/max/min as median because it is not normal distributed
data["sbp_mean"].fillna(data["sbp_mean"].median(skipna=True), inplace=True)
data["sbp_max"].fillna(data["sbp_max"].median(skipna=True), inplace=True)
data["sbp_min"].fillna(data["sbp_min"].median(skipna=True), inplace=True)



#check dbp
plot = data["dbp_min"].hist(bins=15, color='teal')
plot.set(xlabel='dbp_min')
plt.xlim(0,200)
plt.show()


#replace NA in dbp mean/max/min as median because it is not normal distributed
data["dbp_mean"].fillna(data["dbp_mean"].median(skipna=True), inplace=True)
data["dbp_max"].fillna(data["dbp_max"].median(skipna=True), inplace=True)
data["dbp_min"].fillna(data["dbp_min"].median(skipna=True), inplace=True)


#check temperature
plot = data["temperature_mean"].hist(bins=20, color='teal')
plot.set(xlabel='temperature_mean')
plt.xlim(25,50)
plt.show()


#replace NA in temperature mean/max/min as median because it is not normal distributed
data["temperature_mean"].fillna(data["temperature_mean"].median(skipna=True), inplace=True)
data["temperature_max"].fillna(data["temperature_max"].median(skipna=True), inplace=True)
data["temperature_min"].fillna(data["temperature_min"].median(skipna=True), inplace=True)



#check creatinine
plot = data["creatinine_max"].hist(bins=20, color='teal')
plot.set(xlabel='creatinine_max')
plt.xlim(0,50)
plt.show()
data["creatinine_min"].skew()

#replace NA in creatinine mean/max/min as median because it is not normal distributed
data["creatinine_max"].fillna(data["creatinine_max"].median(skipna=True), inplace=True)
data["creatinine_min"].fillna(data["creatinine_min"].median(skipna=True), inplace=True)


#check wbc
#hrx = data["wbc_max"].hist(bins=200, color='teal')
#hrx.set(xlabel='wbc_max')
#plt.xlim(0,410)
#plt.show()
#data["wbc_max"].skew()

#replace NA in wbc_max mean/max/min as median because it is not normal distributed
data["wbc_max"].fillna(data["wbc_max"].median(skipna=True), inplace=True)
data["wbc_min"].fillna(data["wbc_min"].median(skipna=True), inplace=True)


# In[20]:


#check hemoglobin
plot = data["hemoglobin_min"].hist(bins=20, color='teal')
plot.set(xlabel='hemoglobin_min')
plt.xlim(0,24)
plt.show()
data["hemoglobin_min"].skew()

#replace NA in hemoglobin mean/max/min as median because it is not normal distributed
data["hemoglobin_min"].fillna(data["hemoglobin_min"].median(skipna=True), inplace=True)
data["hemoglobin_max"].fillna(data["hemoglobin_max"].median(skipna=True), inplace=True)



# check sofa_coagulation feature, and replace NA as most count 0
print('sofa coagulation distribution (0=0, 1=1.0, 2=2.0, 3=3.0, 4 = 4.0):')
print(data['sofa_coagulation'].value_counts())
sns.countplot(x='sofa_coagulation', data=data, palette='Set2')#plt.show()

data["sofa_coagulation"].fillna(data['sofa_coagulation'].value_counts().idxmax(), inplace=True)





#check glucose
plot = data["glucose_min"].hist(bins=100, color='teal')
plot.set(xlabel='glucose_min')
plt.xlim(0,600)
plt.show()
data["glucose_min"].skew()

#replace NA in glucose mean/max/min as median because it is not normal distributed
data["glucose_min"].fillna(data["glucose_min"].median(skipna=True), inplace=True)
data["glucose_max"].fillna(data["glucose_max"].median(skipna=True), inplace=True)



#check urineoutput
plot= data["urineoutput"].hist(bins=200, color='teal')
plot.set(xlabel='urineoutput')
plt.xlim(-15000,30000)
plt.show()
data["urineoutput"].skew()

#replace NA in urineoutput mean/max/min as median because it is not normal distributed
data["urineoutput"].fillna(data["urineoutput"].median(skipna=True), inplace=True)



#check pt
plot = data["pt_min"].hist(bins=200, color='teal')
plot.set(xlabel='pt_min')
plt.xlim(0,150)
plt.show()
data["pt_min"].skew()

#replace NA in pt mean/max/min as median because it is not normal distributed
data["pt_min"].fillna(data["pt_min"].median(skipna=True), inplace=True)
data["pt_max"].fillna(data["pt_max"].median(skipna=True), inplace=True)



#check ph
plot = data["ph_min"].hist(bins=7, color='teal')
plot.set(xlabel='ph_min')
plt.xlim(5,9)
plt.show()
data["ph_min"].skew()

#replace NA in ph mean/max/min as median because it is not normal distributed
data["ph_min"].fillna(data["ph_min"].median(skipna=True), inplace=True)
data["ph_max"].fillna(data["ph_max"].median(skipna=True), inplace=True)



#check ast
plot = data["ast_min"].hist(bins=200, color='teal')
plot.set(xlabel='ast_min')
plt.xlim(0,3000)
plt.show()
data["ast_min"].skew()

#replace NA in ast mean/max/min as median because it is not normal distributed
data["ast_min"].fillna(data["ast_min"].median(skipna=True), inplace=True)
data["ast_max"].fillna(data["ast_max"].median(skipna=True), inplace=True)



#check alt
plot = data["alt_min"].hist(bins=200, color='teal')
plot.set(xlabel='alt_min')
plt.xlim(0,3000)
plt.show()
data["alt_min"].skew()

#replace NA in ast mean/max/min as median because it is not normal distributed
data["alt_min"].fillna(data["alt_min"].median(skipna=True), inplace=True)
data["alt_max"].fillna(data["alt_max"].median(skipna=True), inplace=True)




#check bilirubin_total
plot = data["bilirubin_total_min"].hist(bins=200, color='teal')
plot.set(xlabel='bilirubin_total_min')
plt.xlim(0,30)
plt.show()
data["bilirubin_total_min"].skew()

#replace NA in bilirubin_total mean/max/min as median because it is not normal distributed
data["bilirubin_total_min"].fillna(data["bilirubin_total_min"].median(skipna=True), inplace=True)
data["bilirubin_total_max"].fillna(data["bilirubin_total_max"].median(skipna=True), inplace=True)


# check sofa_liver feature, and replace NA as most count 0
print('sofa_liver distribution (0=0, 1=1.0, 2=2.0, 3=3.0, 4 = 4.0):')
print(data['sofa_liver'].value_counts())
sns.countplot(x='sofa_liver', data=data, palette='Set2')
plt.show()

data["sofa_liver"].fillna(data['sofa_liver'].value_counts().idxmax(), inplace=True)



#check alp
plot = data["alp_min"].hist(bins=300, color='teal')
plot.set(xlabel='alp_min')
plt.xlim(0,2000)
plt.show()
data["alp_min"].skew()

#replace NA in alp mean/max/min as median because it is not normal distributed
data["alp_min"].fillna(data["alp_min"].median(skipna=True), inplace=True)
data["alp_max"].fillna(data["alp_max"].median(skipna=True), inplace=True)




#check lactate
plot = data["lactate_min"].hist(bins=30, color='teal')
plot.set(xlabel='lactate_min')
plt.xlim(0,20)
plt.show()
data["lactate_min"].skew()

#replace NA in alp mean/max/min as median because it is not normal distributed
data["lactate_min"].fillna(data["lactate_min"].median(skipna=True), inplace=True)
data["lactate_max"].fillna(data["lactate_max"].median(skipna=True), inplace=True)




#check calcium
plot = data["calcium_max"].hist(bins=30, color='teal')
plot.set(xlabel='calcium_max')
plt.xlim(0,5)
plt.show()
data["calcium_max"].skew()

#replace NA in alp mean/max/min as median because it is not normal distributed
data["calcium_min"].fillna(data["calcium_min"].median(skipna=True), inplace=True)
data["calcium_max"].fillna(data["calcium_max"].median(skipna=True), inplace=True)


missing(data)



#Encode gender
data.info()
data.gender=data.gender.astype('category').cat.codes
data["gender"] #M = 1, F = 0



#normalization
x = data[data.columns.drop("outcome")]
x_zscore = (x - x.mean())/x.std()
x_zscore.describe()



#check correlation
plt.figure(figsize=(60, 30))
heatmap = sns.heatmap(x_zscore.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);



#drop if correlation is>0.85
data1 = x_zscore.copy()
correlation_matrix = data1.corr().abs()
upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape),k=1).astype(np.bool))
to_drop = [column for column in upper_tri.columns if any(upper_triangle[column] > 0.85)] 
print(to_drop)
# Drop features 
data1.drop(to_drop, axis=1, inplace=True)
data1.describe().transpose()





###splitting training and testing data using SMOTE

from imblearn.over_sampling import SMOTE

X_df = data1.copy() #already normalized, and drop missing>0.75
y_df = dataset['outcome']
y_df.describe()
X_df.describe()


# import packages

# PyTorch package and submodules
import torch
import torch.nn as nn
from torch.optim import SGD #gradient descent optimizer

# NumPy for math operations, and Pandas for processing tabular data.
import numpy as np
import pandas as pd

# Plotly plotting package
import plotly.graph_objects as go
import plotly.express as px

# We use toy datasets in scikit-learn package
from sklearn.datasets import load_breast_cancer

# We use AUROC and average precision (AP) scores from sklearn
from sklearn.metrics import roc_auc_score, average_precision_score


#change data frame to tensor
X= torch.tensor(X_df.to_numpy(),dtype=torch.float32)
m,n = X.shape
y= torch.tensor(y_df.to_numpy(),dtype=torch.float32).reshape(m,1)
m

n


# We use an approx 6:4 train test splitting
np.random.seed(23)

cases = ['train','test']
case_list = np.random.choice(cases,size=X.shape[0],replace=True,p=[0.6,0.4])

X_df_Train, y_df_Train = X_df.iloc[case_list=='train'], y_df.iloc[case_list=='train']
X_df_Test, y_df_Test = X_df.iloc[case_list=='test'], y_df.iloc[case_list=='test']


# Use SMOTE to resample
smote_sample = SMOTE(random_state=23,sampling_strategy='minority')
X_df_SMOTE, y_df_SMOTE = smote_sample.fit_resample(X_df_Train, y_df_Train)

print("origianl outcome '1': {}".format(sum(y_df_Train==1)))
print("original outcome '0': {} \n".format(sum(y_df_Train==0)))

print('After SMOTE, shape of train_X: {}'.format(X_df_SMOTE.shape))
print('After SMOTE, shape of train_y: {} \n'.format(y_df_SMOTE.shape))

print("After SMOTE, outcome '1': {}".format(sum(y_df_SMOTE==1)))
print("After SMOTE, outcome'0': {}".format(sum(y_df_SMOTE==0)))



y_df_Train.value_counts()

y_df_SMOTE.value_counts()



###model & feature selection

from sklearn.linear_model import LogisticRegression as logit # use build-in logistic regression model in sklearn
from sklearn.feature_selection import SequentialFeatureSelector as SFS
from sklearn.metrics import roc_curve, precision_recall_curve


#original logistic regression
model_SMOTE = logit(solver='liblinear')
model_SMOTE.fit(X_df_SMOTE,y_df_SMOTE)

pred_SMOTE = model_SMOTE.predict_proba(X_df_Test)


results = pd.DataFrame(
    {
        'pred':np.vstack([pred_SMOTE])[:,1],
        'label':pd.concat([y_df_Test]*1),
        'method':['SMOTE']*pred_SMOTE.shape[0]
    }
)

threshold = 0.5

results['binary pred'] = (results['pred']>threshold).astype('int')



results.groupby(['label','method'])['binary pred'].value_counts().unstack()


# In[197]:


from sklearn.model_selection import cross_val_score
kfold_accuracy = cross_val_score(estimator = model_SMOTE, X = X_df_SMOTE, y = y_df_SMOTE, cv = 10)
print('10-Fold Validation Mean Accuracy: {:.2f} %'.format(kfold_accuracy.mean()*100))



from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score
from sklearn.model_selection import cross_val_score

Y_pred = model_SMOTE.predict(X_df_Test)
ROC = roc_auc_score(y_df_Test, Y_pred) 
print('ROC AUC Score: {:.2f}'.format(ROC))

precision_score = precision_score(y_df_Test, Y_pred) 
print('Precision  : {:.2f}'.format(precision_score ))

F1_score = f1_score(y_df_Test, Y_pred) 
print('F1 : {:.2f}'.format(F1_score ))


#dataframe to tensor 
x_ts_smote = torch.tensor(X_df_SMOTE.to_numpy(),dtype=torch.float32)
m,n = x_ts_smote.shape
y_ts_smote = torch.tensor(y_df_SMOTE.to_numpy(),dtype=torch.float32).reshape(m,1)


#original logistic regression
h = torch.nn.Linear(
    in_features=n,
    out_features=1,
    bias=True
)
sigma = torch.nn.Sigmoid()

# Logistic model is linear+sigmoid
f = torch.nn.Sequential(
    h,
    sigma
)

J_BCE = torch.nn.BCELoss()
# We use the Adam optimizer, which is
# a variant of gradient descent method with momentum.
GD_optimizer = torch.optim.Adam(lr=0.01,params=f.parameters())

nIter = 5000
printInterval = 500

for i in range(nIter):
    GD_optimizer.zero_grad()
    pred = f(x_ts_smote)
    loss = J_BCE(pred,y_ts_smote)
    loss.backward()
    GD_optimizer.step()
    if i == 0 or ((i+1)%printInterval) == 0:
        print('Iter {}: average BCE loss is {:.3f}'.format(i+1,loss.item()))

with torch.no_grad():
    pred_test = f(x_ts_smote)

auroc = roc_auc_score(y_ts_smote,pred_test)
ap = average_precision_score(y_ts_smote,pred_test)
print('On test dataset: AUROC {:.3f}, AP {:.3f}'.format(auroc,ap))
        


weight = h.weight.detach().squeeze().clone()


#dataframe to tensor 
x_ts_test = torch.tensor(X_df_Test.to_numpy(),dtype=torch.float32)
m,n = x_ts_test.shape
y_ts_test = torch.tensor(y_df_Test.to_numpy(),dtype=torch.float32).reshape(m,1)


# Test on test data

threshold = 0.5

with torch.no_grad():
    pred_test = f(x_ts_test)


binary_pred = np.where(pred_test.squeeze()>threshold,'True','False')
label = np.where(y_ts_test.squeeze()>0.5,'True','False')
acc = (binary_pred==label).sum()/binary_pred.shape[0]
print('Accuracy on test dataset is {:.2f}%'.format(acc*100))



###logistic regression with L2 regularization

h_L2 = torch.nn.Linear(
    in_features=n,
    out_features=1,
    bias=True
)
sigma = torch.nn.Sigmoid()

# Logistic model is linear+sigmoid
f_L2 = torch.nn.Sequential(
    h_L2,
    sigma
)

J_BCE = torch.nn.BCELoss()

# PyTorch optimizer support L2 regularization by
# setting the weight_decay parameter, which corresponds to
# the regularization strength.
GD_optimizer = torch.optim.Adam(lr=0.01,params=f_L2.parameters(),weight_decay=0.05)

nIter = 500
printInterval = 50

for i in range(nIter):
    GD_optimizer.zero_grad()
    pred = f_L2(x_ts_smote)
    loss = J_BCE(pred,y_ts_smote)
    loss.backward()
    GD_optimizer.step()
    if i == 0 or ((i+1)%printInterval) == 0:
        print('Iter {}: average BCE loss is {:.3f}'.format(i+1,loss.item()))

with torch.no_grad():
    pred_test = f_L2(x_ts_test)

auroc = roc_auc_score(y_ts_test,pred_test)
ap = average_precision_score(y_ts_test,pred_test)
print('On test dataset: AUROC {:.3f}, AP {:.3f}'.format(auroc,ap))

weight_L2 = h_L2.weight.detach().squeeze().clone()


#logistic regression with L1 regularization

h_L1 = torch.nn.Linear(
    in_features=n,
    out_features=1,
    bias=True
)
sigma = torch.nn.Sigmoid()

# Logistic model is linear+sigmoid
f_L1 = torch.nn.Sequential(
    h_L1,
    sigma
)

J_BCE = torch.nn.BCELoss()

GD_optimizer = torch.optim.Adam(lr=0.01,params=f_L1.parameters())

# Define L_1 regularization
def L1_reg(model,lbd):
    result = torch.tensor(0)
    for param in model.parameters(): # iterate over all parameters of our model
        result = result + param.abs().sum()

    return lbd*result


nIter = 500
printInterval = 50
lbd = 0.03 # L1 reg strength

for i in range(nIter):
    GD_optimizer.zero_grad()
    pred = f_L1(x_ts_smote)
    loss = J_BCE(pred,y_ts_smote)
    (loss+L1_reg(f_L1,lbd)).backward()
    GD_optimizer.step()
    if i == 0 or ((i+1)%printInterval) == 0:
        print('Iter {}: average BCE loss is {:.3f}'.format(i+1,loss.item()))

with torch.no_grad():
    pred_test = f_L1(x_ts_test)

auroc = roc_auc_score(y_ts_test,pred_test)
ap = average_precision_score(y_ts_test,pred_test)
print('On test dataset: AUROC {:.3f}, AP {:.3f}'.format(auroc,ap))

weight_L1 = h_L1.weight.detach().squeeze().clone()

weight_df = pd.DataFrame(
    {
        'vanilla':weight,
        'L2':weight_L2,
        'L1':weight_L1
    }
).melt(id_vars=[],value_vars=['vanilla','L2','L1'])
weight_df


fig = px.box(
    weight_df,
    y='value',
    facet_col='variable',
    color='variable',
    points='all',
    title='Logistic Regression Weights Distributions'
)
fig.update_yaxes(
    matches=None,
    showticklabels=True
)
fig.update_traces(jitter=0.5)


weight_L1_0 = pd.DataFrame(
    {
        'L1':abs(weight_L1-0)>0.01
    }
).melt(id_vars=[],value_vars=['L1'])
weight_L1_0


###logistic regression with L1 regularization

h_L1 = torch.nn.Linear(
    in_features=n,
    out_features=1,
    bias=True
)
sigma = torch.nn.Sigmoid()

# Logistic model is linear+sigmoid
f_L1 = torch.nn.Sequential(
    h_L1,
    sigma
)

J_BCE = torch.nn.BCELoss()

GD_optimizer = torch.optim.Adam(lr=0.01,params=f_L1.parameters())

# Define L_1 regularization
def L1_reg(model,lbd):
    result = torch.tensor(0)
    for param in model.parameters(): # iterate over all parameters of our model
        result = result + param.abs().sum()

    return lbd*result


nIter = 500
printInterval = 50
lbd = 0.01 # L1 reg strength

for i in range(nIter):
    GD_optimizer.zero_grad()
    pred = f_L1(x_ts_smote)
    loss = J_BCE(pred,y_ts_smote)
    (loss+L1_reg(f_L1,lbd)).backward()
    GD_optimizer.step()
    if i == 0 or ((i+1)%printInterval) == 0:
        print('Iter {}: average BCE loss is {:.3f}'.format(i+1,loss.item()))

with torch.no_grad():
    pred_test = f_L1(x_ts_test)

auroc = roc_auc_score(y_ts_test,pred_test)
ap = average_precision_score(y_ts_test,pred_test)
print('On test dataset: AUROC {:.3f}, AP {:.3f}'.format(auroc,ap))
weight_L1_1 = h_L1.weight.detach().squeeze().clone()


weight_df = pd.DataFrame(
    {
        'vanilla':weight,
        'L2':weight_L2,
        'L1':weight_L1,
        'L1_s':weight_L1_1
    }
).melt(id_vars=[],value_vars=['vanilla','L2','L1', 'L1_s'])
weight_df


fig = px.box(
    weight_df,
    y='value',
    facet_col='variable',
    color='variable',
    points='all',
    title='Logistic Regression Weights Distributions'
)
fig.update_yaxes(
    matches=None,
    showticklabels=True
)
fig.update_traces(jitter=0.5)


weight_L1_1_df = pd.DataFrame(
    {
        'L1_s':abs(weight_L1_1-0)>0.01
    }
).melt(id_vars=[],value_vars=['L1_s'])
weight_L1_1_df



#backward and forward selection
model = logit(penalty='l1',C=1/5,solver='liblinear') # c: 1/(strength of L1 regularization)

#Forward feature selection.
forward_selection = SFS(
   model, n_features_to_select=5, direction="forward"
).fit(X_df_SMOTE, y_df_SMOTE)


 #Backward feature selection.
backward_selection = SFS(
    model, n_features_to_select=5, direction="backward"
).fit(X_df_SMOTE, y_df_SMOTE)

forward_selection.get_feature_names_out()


backward_selection.get_feature_names_out()



# Plotly plotting package
import plotly.graph_objects as go
import plotly.express as px

# We use toy datasets in scikit-learn package
from sklearn.datasets import load_breast_cancer

# Tools in sklearn to select best model
from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV

# Decision tree classifier in sklearn
from sklearn.tree import DecisionTreeClassifier as DTC, plot_tree

# We use f1 score to test model performance
from sklearn.metrics import f1_score

# Import matplotlib.pyplot to visualize tree models
import matplotlib.pyplot as plt


model.fit(X_df_SMOTE, y_df_SMOTE)
y_pred_full = model.predict_proba(X_df_Test)


# Model with forward selected features
model.fit(forward_selection.transform(X_df_SMOTE),y_df_SMOTE)
y_pred_FS = model.predict_proba(forward_selection.transform(X_df_Test))


# Model with backward selected features
model.fit(backward_selection.transform(X_df_SMOTE),y_df_SMOTE)
y_pred_BS = model.predict_proba(backward_selection.transform(X_df_Test))


# roc_curve
fpr_full, tpr_full, _ = roc_curve(y_df_Test,y_pred_full[:,1])
fpr_FS, tpr_FS, _ = roc_curve(y_df_Test,y_pred_FS[:,1])
fpr_BS, tpr_BS, _ = roc_curve(y_df_Test,y_pred_BS[:,1])

roc_df = pd.DataFrame(
    {
        'False Positive Rate':np.hstack([fpr_full,fpr_FS,fpr_BS]),
        'True Positive Rate':np.hstack([tpr_full,tpr_FS,tpr_BS]),
        'method':['full_model']*len(fpr_full)+['FS']*len(fpr_FS)+['BS']*len(fpr_BS)
    }
)



roc_full = roc_auc_score(y_df_Test,y_pred_full[:,1])
roc_fw = roc_auc_score(y_df_Test,y_pred_FS[:,1])
roc_bw = roc_auc_score(y_df_Test,y_pred_BS[:,1])



print('ROC AUC Score: {:.2f}'.format(roc_full))
print('ROC AUC Score for forward selection: {:.2f}'.format(roc_fw))
print('ROC AUC Score for backward selection: {:.2f}'.format(roc_bw))


#  ROC curve
fig = px.line(roc_df,y='True Positive Rate',x='False Positive Rate',facet_col='method',color='method')
fig



# precision recall curves
p_full, r_full, _ = precision_recall_curve(y_df_Test,y_pred_full[:,1])
p_FS, r_FS, _ = precision_recall_curve(y_df_Test,y_pred_FS[:,1])
p_BS, r_BS, _ = precision_recall_curve(y_df_Test,y_pred_BS[:,1])

pr_df = pd.DataFrame(
    {
        'Precision':np.hstack([p_full,p_FS,p_BS]),
        'Recall':np.hstack([r_full,r_FS,r_BS]),
        'method':['Full Model']*len(p_full)+['Forward Selection']*len(p_FS)+['Backward Selection']*len(p_BS)
    }
)

# Visualize precision recall curve
fig = px.line(pr_df,x='Recall',y='Precision',facet_col='method',color='method')
fig

#Forward feature selection.
forward_selection_10 = SFS(
   model, n_features_to_select=10, direction="forward"
).fit(X_df_SMOTE, y_df_SMOTE)


forward_selection_10.get_feature_names_out()


 #Backward feature selection.
backward_selection_10 = SFS(
    model, n_features_to_select=10, direction="backward"
).fit(X_df_SMOTE, y_df_SMOTE)


backward_selection_10.get_feature_names_out()


model.fit(X_df_SMOTE, y_df_SMOTE)
y_pred_full_10 = model.predict_proba(X_df_Test)


# Model with forward selected features
model.fit(forward_selection_10.transform(X_df_SMOTE),y_df_SMOTE)
y_pred_FS_10 = model.predict_proba(forward_selection_10.transform(X_df_Test))


# Model with backward selected features
model.fit(backward_selection_10.transform(X_df_SMOTE),y_df_SMOTE)
y_pred_BS_10 = model.predict_proba(backward_selection_10.transform(X_df_Test))


# roc_curve
fpr_full_10, tpr_full_10, _ = roc_curve(y_df_Test,y_pred_full_10[:,1])
fpr_FS_10, tpr_FS_10, _ = roc_curve(y_df_Test,y_pred_FS_10[:,1])
fpr_BS_10, tpr_BS_10, _ = roc_curve(y_df_Test,y_pred_BS_10[:,1])

roc_df_10 = pd.DataFrame(
    {
        'False Positive Rate':np.hstack([fpr_full_10,fpr_FS_10,fpr_BS_10]),
        'True Positive Rate':np.hstack([tpr_full_10,tpr_FS_10,tpr_BS_10]),
        'method':['full_model']*len(fpr_full_10)+['FS']*len(fpr_FS_10)+['BS']*len(fpr_BS_10)
    }
)



roc_full_10 = roc_auc_score(y_df_Test,y_pred_full_10[:,1])
roc_fw_10 = roc_auc_score(y_df_Test,y_pred_FS_10[:,1])
roc_bw_10 = roc_auc_score(y_df_Test,y_pred_BS_10[:,1])



print('ROC AUC Score: {:.2f}'.format(roc_full_10))
print('ROC AUC Score for forward selection: {:.2f}'.format(roc_fw_10))
print('ROC AUC Score for backward selection: {:.2f}'.format(roc_bw_10))



# Visualize ROC curve
fig_10 = px.line(roc_df_10,y='True Positive Rate',x='False Positive Rate',facet_col='method',color='method')
fig_10



# precision recall curves
p_full_10, r_full_10, _ = precision_recall_curve(y_df_Test,y_pred_full_10[:,1])
p_FS_10, r_FS_10, _ = precision_recall_curve(y_df_Test,y_pred_FS_10[:,1])
p_BS_10, r_BS_10, _ = precision_recall_curve(y_df_Test,y_pred_BS_10[:,1])

pr_df_10 = pd.DataFrame(
    {
        'Precision':np.hstack([p_full_10,p_FS_10,p_BS_10]),
        'Recall':np.hstack([r_full_10,r_FS_10,r_BS_10]),
        'method':['Full Model']*len(p_full_10)+['Forward Selection']*len(p_FS_10)+['Backward Selection']*len(p_BS_10)
    }
)

# Visualize precision recall curve
fig = px.line(pr_df_10,x='Recall',y='Precision',facet_col='method',color='method')
fig


#Forward feature selection.
forward_selection_15 = SFS(
    model, n_features_to_select=15, direction="forward"
).fit(X_df_SMOTE, y_df_SMOTE)


forward_selection_15.get_feature_names_out()


 #Backward feature selection.
backward_selection_15 = SFS(
    model, n_features_to_select=15, direction="backward"
).fit(X_df_SMOTE, y_df_SMOTE)


backward_selection_15.get_feature_names_out()

model.fit(X_df_SMOTE, y_df_SMOTE)
y_pred_full_15 = model.predict_proba(X_df_Test)

# Model with forward selected features
model.fit(forward_selection_15.transform(X_df_SMOTE),y_df_SMOTE)
y_pred_FS_15 = model.predict_proba(forward_selection_15.transform(X_df_Test))


# Model with backward selected features
model.fit(backward_selection_15.transform(X_df_SMOTE),y_df_SMOTE)
y_pred_BS_15 = model.predict_proba(backward_selection_15.transform(X_df_Test))

# roc_curve
fpr_full_15, tpr_full_15, _ = roc_curve(y_df_Test,y_pred_full_15[:,1])
fpr_FS_15, tpr_FS_15, _ = roc_curve(y_df_Test,y_pred_FS_15[:,1])
fpr_BS_15, tpr_BS_15, _ = roc_curve(y_df_Test,y_pred_BS_15[:,1])

roc_df_15 = pd.DataFrame(
    {
        'False Positive Rate':np.hstack([fpr_full_15,fpr_FS_15,fpr_BS_15]),
        'True Positive Rate':np.hstack([tpr_full_15,tpr_FS_15,tpr_BS_15]),
        'method':['full_model']*len(fpr_full_15)+['FS']*len(fpr_FS_15)+['BS']*len(fpr_BS_15)
    }
)

roc_full_15 = roc_auc_score(y_df_Test,y_pred_full_15[:,1])
roc_fw_15 = roc_auc_score(y_df_Test,y_pred_FS_15[:,1])
roc_bw_15 = roc_auc_score(y_df_Test,y_pred_BS_15[:,1])



print('ROC AUC Score: {:.2f}'.format(roc_full_15))
print('ROC AUC Score for forward selection: {:.2f}'.format(roc_fw_15))
print('ROC AUC Score for backward selection: {:.2f}'.format(roc_bw_15))


#Backward feature selection.
backward_selection_ = SFS(
    model, n_features_to_select=15, direction="backward"
).fit(X_df_SMOTE, y_df_SMOTE)

#AUC does not increase a lot when increase features from 10 to 15
#try 15 first



# random forest and feature importance
# Import libraries
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier



rf = RandomForestClassifier( random_state=0)


rf = rf.fit(X_df_SMOTE, y_df_SMOTE)


importances = rf.feature_importances_


# visualize feature importance

plt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')

importances = pd.Series(rf.feature_importances_, index= X_df_SMOTE.columns)

importances.nlargest(25).plot(kind='barh')


y_pred_rf = rf.predict(X_df_Test)


# Check accuracy score 

from sklearn.metrics import accuracy_score

print('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_df_Test, y_pred_rf)))


rf1 = RandomForestClassifier(n_estimators=100, random_state=0)


rf1 = rf1.fit(X_df_SMOTE, y_df_SMOTE)




# visualize feature importance

plt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')

importances = pd.Series(rf1.feature_importances_, index= X_df_SMOTE.columns)

importances.nlargest(15).plot(kind='barh')


y_pred_rf1 = rf1.predict(X_df_Test)



# Check accuracy score 

from sklearn.metrics import accuracy_score

print('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_df_Test, y_pred_rf1)))




####variable set 3

#Build Random forest based on selected variables (top 15 imprtances)
data2 = X_df_SMOTE.copy()
X_df_SMOTE_15rf =data2[['sofa_cns','bilirubin_total_max','gender','sofa_renal','ast_min','ph_min','lactate_min','alt_min','alt_max','charlson_comorbidity_index','calcium_max','age','ph_max','creatinine_min','alp_max']]
X_df_SMOTE_15rf.head(10)
data3 = X_df_Test.copy()
X_df_Test_15rf = data3[['sofa_cns','bilirubin_total_max','gender','sofa_renal','ast_min','ph_min','lactate_min','alt_min','alt_max','charlson_comorbidity_index','calcium_max','age','ph_max','creatinine_min','alp_max']]
X_df_Test_15rf.describe()


rf_15 = RandomForestClassifier(random_state=0)

rf_15.fit(X_df_SMOTE_15rf, y_df_SMOTE)

# Predict on the test set results
y_pred_15rf = rf_15.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_15rf))
accuracies = cross_val_score(estimator = rf_15, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_15rf)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_15rf)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_15rf)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_15rf)  #Confusion Matrix
print(con_matrix)

#grid search
parameters_randf = { 
    'n_estimators': [50,100,200],
    'max_depth' : [4,5,6,7,8,10,11],
    'criterion' :['entropy']
}


stratifiedCV = StratifiedKFold(n_splits=5)
randomforest = RandomForestClassifier(random_state=0)
Bestrandomforest = GridSearchCV(
    randomforest,
    param_grid=parameters_randf,
    scoring='f1',
    cv=stratifiedCV
)
Bestrandomforest.fit(X_df_SMOTE_15rf,y_df_SMOTE)
Bestrandomforest.best_params_
Bestrandomforest.best_score_
y_pred_rfgrid = Bestrandomforest.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_rfgrid))
accuracies = cross_val_score(estimator =  RandomForestClassifier(criterion = 'entropy', n_estimators =200,max_depth = 11, random_state=0), X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_rfgrid)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_rfgrid)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_rfgrid)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_rfgrid)  #Confusion Matrix
print(con_matrix)

###change max depth
rf_15_mdf15 = RandomForestClassifier(criterion = 'entropy', n_estimators =20,max_depth = 5, random_state=0)
rf_15_mdf15.fit(X_df_SMOTE_15rf, y_df_SMOTE)
y_pred_mdf15 = rf_15_mdf15.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_mdf15))
accuracies = cross_val_score(estimator = rf_15_mdf15, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_mdf15)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_mdf15)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_mdf15)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_mdf15)  #Confusion Matrix
print(con_matrix)



#logistic
logreg = LogisticRegression()
logreg.fit(X_df_SMOTE_13L1, y_df_SMOTE)
y_pred = logreg.predict(X_df_Test_13L1)
y_pred_proba = logreg.predict_proba(X_df_Test_13L1)[:, 1]
a = accuracy_score(y_df_Test, y_pred)
kfold_accuracy = cross_val_score(estimator = model_SMOTE_rf, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print('K-Fold Validation Mean Accuracy: {:.2f} %'.format(kfold_accuracy.mean()*100))

from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve,plot_precision_recall_curve, average_precision_score
Y_pred_rf = model_SMOTE_rf.predict(X_df_Test_15rf)
ROC = roc_auc_score(y_df_Test, Y_pred_rf) 
print('ROC AUC Score: {:.2f}'.format(ROC))
precision_score = precision_score(y_df_Test,Y_pred_rf) 
print('Precision Score : {:.2f}'.format(precision_score ))

F1_score = f1_score(y_df_Test, Y_pred_rf) 
print('F1 Score : {:.2f}'.format(F1_score ))

con_matrix = confusion_matrix(y_df_Test, Y_pred_rf)  #Confusion Matrix
print(con_matrix)



###decision tree
# Decision tree classifier in sklearn
from sklearn.tree import DecisionTreeClassifier as DTC, plot_tree
TreeModel = DTC(criterion='entropy',max_depth=5,random_state=15)
TreeModel.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_pred_treemodel = TreeModel.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_treemodel))
accuracies = cross_val_score(estimator = DTC(criterion='entropy',max_depth=5,random_state=15), X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_treemodel)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_treemodel)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_treemodel)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_treemodel)  #Confusion Matrix
print(con_matrix)


# Splitting rules can be visualized by using plot_tree in sklearn
plt.figure(figsize=(14,10))
plot_tree(
    TreeModel,
    filled=True,
    feature_names=['sofa_cns','bilirubin_total_max','gender','sofa_renal','ast_min','ph_min','lactate_min','alt_min','alt_max','charlson_comorbidity_index','calcium_max','age','ph_max','creatinine_min','alp_max'],
    class_names=['True','FALSE']
)
plt.show()


# The `max_depth` parameter is important for decision tree
# We use `GridSearchCV` to select the best `max_depth`

parameters = {'max_depth':np.arange(start=1,stop=10,step=1)}
stratifiedCV = StratifiedKFold(n_splits=8)
TreeModel = DTC(criterion='entropy')
BestTree = GridSearchCV(
    TreeModel,
    param_grid=parameters,
    scoring='f1',
    cv=stratifiedCV
)
BestTree.fit(X_df_SMOTE_15rf,y_df_SMOTE)
BestTree.best_estimator_
###best depth = 9
BestTree.best_score_
y_pred_desTree = BestTree.predict(X_df_Test_15rf)
accuracies = cross_val_score(estimator = DTC(criterion='entropy', max_depth=9), X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_desTree))
f1 = f1_score(y_df_Test, y_pred_desTree)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_desTree)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_desTree)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_desTree)  #Confusion Matrix
print(con_matrix)


#try maxdepth = 15
TreeModel_15 = DTC(criterion='entropy',max_depth=15,random_state=15)
TreeModel_15.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_pred_15 = TreeModel_15.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_15))
accuracies = cross_val_score(estimator = DTC(criterion='entropy', max_depth=15), X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_15)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_15)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_15)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_15)  #Confusion Matrix
print(con_matrix)



#XGBOOST

from xgboost import XGBClassifier as XGBC

parameters = {
    'n_estimators':np.arange(start=2,stop=20,step=2),
    'max_depth':np.arange(start=2,stop=6,step=1),
    'learning_rate':np.arange(start=0.05,stop=0.4,step=0.05)
}
stratifiedCV = StratifiedKFold(n_splits=8)
# XGBC: XGBoost classifier
XGBoostModel = XGBC()
BestXGBoost = GridSearchCV(
    XGBoostModel,
    param_grid=parameters,
    scoring='f1',
    cv=stratifiedCV,
    verbose=1,
    n_jobs=-1 # use all cpu cores to speedup grid search
)
BestXGBoost.fit(X_df_SMOTE_15rf,y_df_SMOTE)
BestXGBoost.best_params_
BestXGBoost.best_score_
y_pred_xgboost = BestXGBoost.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_xgboost))
accuracies = cross_val_score(estimator =XGBC(learning_rate=0.35000000000000003, max_depth=5, n_estimators=18), X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_xgboost)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_xgboost)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_xgboost)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_xgboost)  #Confusion Matrix
print(con_matrix)


#defult
mod = XGBC(random_state=0)
mod.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_mod = mod.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_mod))
accuracies = cross_val_score(estimator = mod, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_mod)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_mod)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_mod)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_mod)  #Confusion Matrix
print(con_matrix)


#try 15
mod15 = XGBC(learning_rate=0.5, max_depth=9, n_estimators=18,random_state=0)
mod15.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_mod1 = mod15.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_mod1))
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score
f1 = f1_score(y_df_Test, y_mod1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_mod1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_mod1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_mod1)  #Confusion Matrix
print(con_matrix)




# Support Vector Classifier
from sklearn.svm import SVC
# 'C': strength of L2 regularization on linear SVM. Larger 'C' --> smaller regularization.
parameters = {
    'C':np.arange(start=1,stop=20,step=5)
}
stratifiedCV = StratifiedKFold(n_splits=8)
SVCModel = SVC(kernel='linear')
BestSVC = GridSearchCV(
    SVCModel,
    param_grid=parameters,
    scoring='f1',
    cv=stratifiedCV,
    verbose=1,
    n_jobs=-1
)
BestSVC.fit(X_df_SMOTE_15rf,y_df_SMOTE)
BestSVC.best_estimator_
BestSVC.best_score_
y_pred_svc = BestSVC.predict(X_df_Test_15rf)
accuracies = cross_val_score(estimator =SVC(C=6, kernel='linear'), X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_svc))
f1 = f1_score(y_df_Test, y_pred_svc)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_svc)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_svc)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_svc)  #Confusion Matrix
print(con_matrix)


###AdaBoost
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
ada = AdaBoostClassifier(DecisionTreeClassifier(), random_state=0)
ada.fit(X_df_SMOTE_15rf,y_df_SMOTE)
print('Accuracy Score in Train data: ',ada.score(X_df_SMOTE_15rf,y_df_SMOTE))
y_pred_ada = ada.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_ada))
accuracies = cross_val_score(estimator =ada, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test,  y_pred_ada)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_ada)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_ada)  #Confusion Matrix
print(con_matrix)


##use decision tree from gridsearch DTC
ada_dtree = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=9),random_state=0)
ada_dtree.fit(X_df_SMOTE_15rf,y_df_SMOTE)
print('Accuracy Score in Train data: ',ada_dtree.score(X_df_SMOTE_15rf,y_df_SMOTE))
y_pred_ada_dtree = ada_dtree.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_ada_dtree))
accuracies = cross_val_score(estimator =ada_dtree, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada_dtree)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_ada_dtree)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_ada_dtree)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_ada_dtree)  #Confusion Matrix
print(con_matrix)

##try max depth = 15
ada_dtree_15 = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=15),random_state=0)
ada_dtree_15.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_pred_ada_dtree_15 = ada_dtree_15.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_ada_dtree_15))
accuracies = cross_val_score(estimator =ada_dtree_15, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada_dtree_15)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_ada_dtree_15)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_ada_dtree_15)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_ada_dtree_15)  #Confusion Matrix
print(con_matrix)


from sklearn.svm import SVC
nonlinear_models = {
    'DecisionTree':DTC(criterion='entropy'),
    'XGBoost':XGBC(),
    'SVM_rbf':SVC(kernel='rbf')
}

stratifiedCV = StratifiedKFold(n_splits=8)


params = {
    'DecisionTree':{
        'max_depth':np.arange(start=5,stop=15, step = 3)
    },
    'XGBoost':{
        'n_estimators':np.arange(start=2,stop=20,step=2),
        'max_depth':np.arange(start=2,stop=6),
        'learning_rate':np.arange(start=0.05,stop=0.4,step=0.05)
    },
    'SVM_rbf':{
        'C':np.arange(0.5,5,step=0.5)
    }
}

records = {}

for model in nonlinear_models:
    BestParams = GridSearchCV(
        nonlinear_models[model],
        param_grid = params[model],
        scoring='f1',
        cv=stratifiedCV,
        n_jobs=-1
    )
    BestParams.fit(X_df_SMOTE_15rf,y_df_SMOTE)

    print('For {} cross validation F1 score is {:.4f}'.format(model,BestParams.best_score_))







###Varialbe set 2
###Use backward selection 15 variables
data2 = X_df_SMOTE.copy()
X_df_SMOTE_15bw =data2[['heart_rate_min', 'mbp_min', 'dbp_min', 'temperature_max',
       'temperature_mean', 'lactate_max', 'calcium_min', 'calcium_max',
       'glucose_min', 'glucose_max', 'alt_max', 'alp_max', 'urineoutput',
       'sofa_cns', 'sofa_renal']]


X_df_Test_15bw = data3[['heart_rate_min', 'mbp_min', 'dbp_min', 'temperature_max',
       'temperature_mean', 'lactate_max', 'calcium_min', 'calcium_max',
       'glucose_min', 'glucose_max', 'alt_max', 'alp_max', 'urineoutput',
       'sofa_cns', 'sofa_renal']]

#random forest
bw_15 = RandomForestClassifier(random_state=0)
bw_15.fit(X_df_SMOTE_15bw, y_df_SMOTE)

# Predict on the test set results
y_pred_15bw = bw_15.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_15bw))
accuracies = cross_val_score(estimator = bw_15, X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_15bw)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_15bw)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_15bw)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_15bw)  #Confusion Matrix
print(con_matrix)

#grid search CV
parameters_randf_bw = { 
    'n_estimators': [50,100,200],
    'max_depth' : [4,5,6,7,8,10,11,12,13],
    'criterion' :['entropy']
}

stratifiedCV = StratifiedKFold(n_splits=5)
randomforest_bw = RandomForestClassifier(random_state=0)
Bestrandomforest_bw = GridSearchCV(
    randomforest_bw,
    param_grid=parameters_randf_bw,
    scoring='f1',
    cv=stratifiedCV
)
Bestrandomforest_bw.fit(X_df_SMOTE_15bw,y_df_SMOTE)
Bestrandomforest_bw.best_params_
Bestrandomforest_bw.best_score_
y_pred_rf_bw_grid = Bestrandomforest_bw.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_rf_bw_grid))
accuracies = cross_val_score(estimator =RandomForestClassifier(criterion = 'entropy', n_estimators =200,max_depth = 13, random_state=0) , X = X_df_SMOTE_15bw, y =  y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_rf_bw_grid)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_rf_bw_grid)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_rf_bw_grid)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_rf_bw_grid)  #Confusion Matrix
print(con_matrix)


#try depth = 15
bww = RandomForestClassifier(max_depth = 7,random_state=0)
bww.fit(X_df_SMOTE_15bw, y_df_SMOTE)

# Predict on the test set results
y_pred_15bww = bww.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test,y_pred_15bww))
accuracies = cross_val_score(estimator = bww, X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_15bww)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_15bww)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_15bww)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_15bww)  #Confusion Matrix
print(con_matrix)




###decision tree, use backward selected variables
TreeModel_bw = DTC(criterion='entropy',max_depth=5,random_state=15)
TreeModel_bw.fit(X_df_SMOTE_15bw,y_df_SMOTE)
y_pred_treemodel_bw = TreeModel_bw.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_treemodel_bw))
accuracies = cross_val_score(estimator = TreeModel_bw, X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test,y_pred_treemodel_bw)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_treemodel_bw)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_treemodel_bw)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_treemodel_bw)  #Confusion Matrix
print(con_matrix)


parameters_bw = {'max_depth':np.arange(start=1,stop=10,step=1)}
stratifiedCV = StratifiedKFold(n_splits=8)
TreeModel_bw = DTC(criterion='entropy')
BestTree_bw = GridSearchCV(
    TreeModel_bw,
    param_grid=parameters_bw,
    scoring='f1',
    cv=stratifiedCV
)
BestTree_bw.fit(X_df_SMOTE_15bw,y_df_SMOTE)
BestTree_bw.best_estimator_
BestTree_bw.best_score_
y_pred_desTree_bw = BestTree_bw.predict(X_df_Test_15bw)
accuracies = cross_val_score(estimator = DTC(criterion='entropy', max_depth=9), X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test,y_pred_desTree_bw)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_desTree_bw)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_desTree_bw)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test,  y_pred_desTree_bw)  #Confusion Matrix
print(con_matrix)


#try max depth = 15
TreeModel_bw_15 = DTC(criterion='entropy',max_depth=15,random_state=15)
TreeModel_bw_15.fit(X_df_SMOTE_15bw,y_df_SMOTE)

y_pred_desTree_bw_15 = TreeModel_bw_15.predict(X_df_Test_15bw)
accuracies = cross_val_score(estimator = TreeModel_bw_15 , X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_desTree_bw_15)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_desTree_bw_15)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_desTree_bw_15)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_desTree_bw_15)  #Confusion Matrix
print(con_matrix)



####XGBoost
parameters_bw = {
    'n_estimators':np.arange(start=4,stop=30,step=2),
    'max_depth':np.arange(start=2,stop=8,step=1),
    'learning_rate':np.arange(start=0.05,stop=0.4,step=0.05)
}
stratifiedCV = StratifiedKFold(n_splits=8)
# XGBC: XGBoost classifier
XGBoostModel_bw = XGBC()
BestXGBoost_bw = GridSearchCV(
    XGBoostModel_bw,
    param_grid=parameters_bw,
    scoring='f1',
    cv=stratifiedCV,
    verbose=1,
    n_jobs=-1 # use all cpu cores to speedup grid search
)
BestXGBoost_bw.fit(X_df_SMOTE_15bw,y_df_SMOTE)
BestXGBoost_bw.best_params_
BestXGBoost_bw.best_score_
y_pred_xgboost_bw = BestXGBoost_bw.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_xgboost_bw))
accuracies = cross_val_score(estimator =XGBC(learning_rate=0.35000000000000003, max_depth=7, n_estimators=28), X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test,y_pred_xgboost_bw)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_xgboost_bw)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_xgboost_bw)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test,  y_pred_xgboost_bw)  #Confusion Matrix
print(con_matrix)



#try default
mod1 = XGBC(random_state=0)
mod1.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_mod1 = mod1.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_mod1))
accuracies = cross_val_score(estimator = mod1, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_mod1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_mod1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_mod1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_mod1)  #Confusion Matrix
print(con_matrix)


#try 15
mod15 = XGBC(learning_rate=0.35000000000000003, max_depth=15, n_estimators=28,random_state=0)
mod15.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_mod1 = mod15.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_mod1))
accuracies = cross_val_score(estimator = mod15, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_mod1)  #F1 Score
print('F1: {:.2f}'.format(f1))
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score
precision_score = precision_score(y_df_Test,y_mod1) 
print('Precision Score : {:.2f}'.format(precision_score ))
roc = roc_auc_score(y_df_Test,  y_mod1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_mod1)  #Confusion Matrix
print(con_matrix)



###AdaBoost
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
ada_bw = AdaBoostClassifier(DecisionTreeClassifier(), random_state=0)
y_pred_ada_bw = ada_bw.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_ada_bw))
accuracies = cross_val_score(estimator =ada_bw, X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada_bw)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test,y_pred_ada_bw)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_ada_bw)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test,  y_pred_ada_bw)  #Confusion Matrix
print(con_matrix)


##use decision tree from gridsearch DTC
ada_dtree_bw = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=9),random_state=0)
ada_dtree_bw.fit(X_df_SMOTE_15bw,y_df_SMOTE)
print('Accuracy Score in Train data: ',ada_dtree_bw.score(X_df_SMOTE_15bw,y_df_SMOTE))
y_pred_ada_dtree_bw = ada_dtree_bw.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_ada_dtree_bw))
accuracies = cross_val_score(estimator =ada_dtree_bw, X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada_dtree_bw)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test,y_pred_ada_dtree_bw)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_ada_dtree_bw)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test,  y_pred_ada_dtree_bw)  #Confusion Matrix
print(con_matrix)



#try max depth = 15
ada_dtree_bw_15 = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=15),random_state=0)
ada_dtree_bw_15.fit(X_df_SMOTE_15bw,y_df_SMOTE)
y_pred_ada_dtree_bw_15 = ada_dtree_bw_15.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_ada_dtree_bw_15))
accuracies = cross_val_score(estimator =ada_dtree_bw_15, X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada_dtree_bw_15)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_ada_dtree_bw_15)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_ada_dtree_bw_15)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_ada_dtree_bw_15)  #Confusion Matrix
print(con_matrix)





###SVM
# 'C': strength of L2 regularization on linear SVM. Larger 'C' --> smaller regularization.
parameters_bw = {
    'C':np.arange(start=1,stop=20,step=5)
}
stratifiedCV = StratifiedKFold(n_splits=8)
SVCModel_bw = SVC(kernel='linear')
BestSVC_bw = GridSearchCV(
    SVCModel_bw,
    param_grid=parameters_bw,
    scoring='f1',
    cv=stratifiedCV,
    verbose=1,
    n_jobs=-1
)
BestSVC_bw.fit(X_df_SMOTE_15bw,y_df_SMOTE)
BestSVC_bw.best_estimator_
BestSVC_bw.best_score_
y_pred_svc_bw = BestSVC_bw.predict(X_df_Test_15bw)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_svc_bw))
accuracies = cross_val_score(estimator =SVC(C=6, kernel='linear'), X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_svc_bw)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test,y_pred_svc_bw)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test, y_pred_svc_bw)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_svc_bw)  #Confusion Matrix
print(con_matrix)


#logistic regression
model_SMOTE_bw = logit(solver='liblinear')
model_SMOTE_bw.fit(X_df_SMOTE_15bw,y_df_SMOTE)
pred_SMOTE_bw = model_SMOTE_bw.predict_proba(X_df_Test_15bw)
kfold_accuracy = cross_val_score(estimator = model_SMOTE_bw, X = X_df_SMOTE_15bw, y = y_df_SMOTE, cv = 10)
print('10-Fold Validation Mean Accuracy: {:.2f} %'.format(kfold_accuracy.mean()*100))
Y_pred_bw = model_SMOTE_bw.predict(X_df_Test_15bw)
ROC = roc_auc_score(y_df_Test,Y_pred_bw ) 
print('ROC AUC Score: {:.2f}'.format(ROC))
precision_score = precision_score(y_df_Test, Y_pred_bw) 
print('Precision Score : {:.2f}'.format(precision_score ))
F1 = f1_score(y_df_Test, Y_pred_bw) 
print('F1 Score : {:.2f}'.format(F1))



###variable set 1
####Try the 13 variables from L1 regularization
data2 = X_df_SMOTE.copy()
X_df_SMOTE_13L1 =data2[['gender','heart_rate_min', 'dbp_max', 'temperature_min',
        'lactate_max',  'calcium_max',
       'glucose_min', 'glucose_max','creatinine_min', 'alp_max', 'urineoutput',
       'sofa_cns', 'sofa_liver']]

X_df_Test_13L1 = data3[['gender','heart_rate_min', 'dbp_max' ,'temperature_min',
        'lactate_max',  'calcium_max',
       'glucose_min', 'glucose_max','creatinine_min', 'alp_max', 'urineoutput',
       'sofa_cns', 'sofa_liver']]

#random forest
L1_13 = RandomForestClassifier(random_state=0)
L1_13.fit(X_df_SMOTE_13L1, y_df_SMOTE)
y_pred_13L1 = L1_13.predict(X_df_Test_13L1)
print('Accuracy Score: ',accuracy_score(y_df_Test,y_pred_13L1))
accuracies = cross_val_score(estimator = L1_13, X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_13L1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_13L1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_13L1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_13L1)  #Confusion Matrix
print(con_matrix)
#grid search CV
parameters_randf_L1 = { 
    'n_estimators': [50,100,200],
    'max_depth' : [4,5,6,7,8,10,11,12,13],
    'criterion' :['entropy']
}

stratifiedCV = StratifiedKFold(n_splits=5)
randomforest_L1 = RandomForestClassifier(random_state=0)
Bestrandomforest_L1 = GridSearchCV(
    randomforest_L1,
    param_grid=parameters_randf_L1,
    scoring='f1',
    cv=stratifiedCV
)
Bestrandomforest_L1.fit(X_df_SMOTE_13L1,y_df_SMOTE)
Bestrandomforest_L1.best_params_
Bestrandomforest_L1.best_score_
y_pred_rf_L1_grid = Bestrandomforest_L1.predict(X_df_Test_13L1)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_rf_L1_grid))
accuracies = cross_val_score(estimator =RandomForestClassifier(criterion = 'entropy', n_estimators =200,max_depth = 13, random_state=0) , X = X_df_SMOTE_13L1, y =  y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_rf_L1_grid)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_rf_L1_grid)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_rf_L1_grid)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test,y_pred_rf_L1_grid)  #Confusion Matrix
print(con_matrix)



#logistic regression
#original logistic regression
model_SMOTE_13 = logit(solver='liblinear')
model_SMOTE_13.fit(X_df_SMOTE_13L1,y_df_SMOTE)
pred_SMOTE_13 = model_SMOTE_13.predict_proba(X_df_Test_13L1)
accuracy = cross_val_score(estimator = model_SMOTE_13, X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print('10-Fold Validation Mean Accuracy: {:.2f} %'.format(accuracy.mean()*100))
Y_pred_13 = model_SMOTE_13.predict(X_df_Test_13L1)
ROC = roc_auc_score(y_df_Test, Y_pred_13) 
print('ROC AUC Score: {:.2f}'.format(ROC))
precision_score = precision_score(y_df_Test,Y_pred_13) 
print('Precision Score : {:.2f}'.format(precision_score ))
F1 = f1_score(y_df_Test, Y_pred_13) 
print('F1 Score : {:.2f}'.format(F1 ))



###decision tree, use L1 13 variables
TreeModel_L1 = DTC(criterion='entropy',max_depth=5,random_state=15)
TreeModel_L1.fit(X_df_SMOTE_13L1,y_df_SMOTE)
y_pred_treemodel_L1 = TreeModel_L1.predict(X_df_Test_13L1)
print('Accuracy Score: ',accuracy_score(y_df_Test,y_pred_treemodel_L1))
accuracies = cross_val_score(estimator = TreeModel_L1, X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_treemodel_L1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_treemodel_L1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_treemodel_L1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix= confusion_matrix(y_df_Test,y_pred_treemodel_L1)  #Confusion Matrix
print(con_matrix)

parameters_L1 = {'max_depth':np.arange(start=1,stop=14,step=2)}
stratifiedCV = StratifiedKFold(n_splits=8)
TreeModel_L1 = DTC(criterion='entropy')
BestTree_L1 = GridSearchCV(
    TreeModel_L1,
    param_grid=parameters_L1,
    scoring='f1',
    cv=stratifiedCV
)
BestTree_L1.fit(X_df_SMOTE_13L1,y_df_SMOTE)
BestTree_L1.best_estimator_
BestTree_L1.best_score_
y_pred_desTree_L1 = BestTree_L1.predict(X_df_Test_13L1)
accuracies = cross_val_score(estimator = DTC(criterion='entropy', max_depth=13), X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_desTree_L1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_desTree_L1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_desTree_L1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test,y_pred_desTree_L1)  #Confusion Matrix
print(con_matrix)



####XGBoost
parameters_L1 = {
    'n_estimators':np.arange(start=4,stop=30,step=2),
    'max_depth':np.arange(start=2,stop=14,step=2),
    'learning_rate':np.arange(start=0.05,stop=0.4,step=0.05)
}
stratifiedCV = StratifiedKFold(n_splits=8)
# XGBC: XGBoost classifier
XGBoostModel_L1 = XGBC()
BestXGBoost_L1 = GridSearchCV(
    XGBoostModel_L1,
    param_grid=parameters_L1,
    scoring='f1',
    cv=stratifiedCV,
    verbose=1,
    n_jobs=-1 # use all cpu cores to speedup grid search
)
BestXGBoost_L1.fit(X_df_SMOTE_13L1,y_df_SMOTE)
BestXGBoost_L1.best_params_
BestXGBoost_L1.best_score_
y_pred_xgboost_L1 = BestXGBoost_L1.predict(X_df_Test_13L1)
accuracies = cross_val_score(estimator =XGBC(learning_rate=0.35000000000000003, max_depth=12, n_estimators=28), X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_xgboost_L1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_xgboost_L1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_xgboost_L1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_xgboost_L1)  #Confusion Matrix
print(con_matrix)



#try max depth = 13
xboost_L1_13=XGBC(learning_rate=0.35000000000000003, max_depth=13, n_estimators=28)
xboost_L1_13.fit(X_df_SMOTE_13L1, y_df_SMOTE)
y_pred_xgboost_L1_13 = xboost_L1_13.predict(X_df_Test_13L1)
accuracies = cross_val_score(estimator =xboost_L1_13, X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_xgboost_L1_13)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_xgboost_L1_13)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_xgboost_L1_13)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_xgboost_L1_13)  #Confusion Matrix
print(con_matrix)

#defult
mod2 = XGBC(random_state=0)
mod2.fit(X_df_SMOTE_15rf,y_df_SMOTE)
y_mod2 = mod2.predict(X_df_Test_15rf)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_mod2))
accuracies = cross_val_score(estimator = mod2, X = X_df_SMOTE_15rf, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_mod2)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_mod2)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_mod2)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_mod2)  #Confusion Matrix
print(con_matrix)



####AdaBoost
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
ada_L1 = AdaBoostClassifier(DecisionTreeClassifier(), random_state=0)
ada_L1.fit(X_df_SMOTE_13L1,y_df_SMOTE)
y_pred_ada_L1 = ada_L1.predict(X_df_Test_13L1)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_ada_L1))
accuracies = cross_val_score(estimator =ada_L1, X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada_L1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_ada_L1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_ada_L1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_ada_L1)  #Confusion Matrix
print(con_matrix)


##use decision tree from gridsearch DTC
ada_dtree_L1 = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=13),random_state=0)
ada_dtree_L1.fit(X_df_SMOTE_13L1,y_df_SMOTE)
y_pred_ada_dtree_L1 = ada_dtree_L1.predict(X_df_Test_13L1)
print('Accuracy Score: ',accuracy_score(y_df_Test,y_pred_ada_dtree_L1))
accuracies = cross_val_score(estimator =ada_dtree_L1, X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_ada_dtree_L1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_ada_dtree_L1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_ada_dtree_L1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_ada_dtree_L1)  #Confusion Matrix
print(con_matrix)




###SVM
# 'C': strength of L2 regularization on linear SVM. Larger 'C' --> smaller regularization.
parameters_L1 = {
    'C':np.arange(start=1,stop=20,step=5)
}
stratifiedCV = StratifiedKFold(n_splits=8)
SVCModel_L1 = SVC(kernel='linear')
BestSVC_L1 = GridSearchCV(
    SVCModel_L1,
    param_grid=parameters_L1,
    scoring='f1',
    cv=stratifiedCV,
    verbose=1,
    n_jobs=-1
)
BestSVC_L1.fit(X_df_SMOTE_13L1,y_df_SMOTE)
BestSVC_L1.best_estimator_
BestSVC_L1.best_score_
y_pred_svc_L1 = BestSVC_L1.predict(X_df_Test_13L1)
print('Accuracy Score: ',accuracy_score(y_df_Test, y_pred_svc_L1))
accuracies = cross_val_score(estimator =SVC(C=6, kernel='linear'), X = X_df_SMOTE_13L1, y = y_df_SMOTE, cv = 10)
print("10-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
f1 = f1_score(y_df_Test, y_pred_svc_L1)  #F1 Score
print('F1: {:.2f}'.format(f1))
precision = precision_score(y_df_Test, y_pred_svc_L1)  #Precision Score
print('Precision: {:.2f}'.format(precision))
roc = roc_auc_score(y_df_Test,  y_pred_svc_L1)  #ROC AUC Score
print('ROC AUC Score: {:.2f}'.format(roc))
con_matrix = confusion_matrix(y_df_Test, y_pred_svc_L1)  #Confusion Matrix
print(con_matrix)
